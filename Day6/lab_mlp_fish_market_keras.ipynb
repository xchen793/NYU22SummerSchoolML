{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab_mlp_fish_market_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPhD3tBWYP9G"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTHIFLwUYP9b"
      },
      "source": [
        "### Load and scale the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOf98xw7YP9d"
      },
      "source": [
        "feature = pd.read_csv('https://github.com/asarmadi/tandon_summer2021_ml/raw/main/day05/fish_market_feature.csv')\n",
        "label = pd.read_csv('https://github.com/asarmadi/tandon_summer2021_ml/raw/main/day05/fish_market_label.csv')\n",
        "X = feature.values\n",
        "y = label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLrMTKvFm4VC"
      },
      "source": [
        "# normalize the data using sklearn's StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "Xs = scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K6BT_RVm4VC"
      },
      "source": [
        "## TODO\n",
        "# split the SCALED!! data in validation and train\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(..., test_size=0.1, random_state=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BZ-u4Owm4VD"
      },
      "source": [
        "## TODO\n",
        "# print the number of data samples in the training and validation data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd-R6M1YP9s"
      },
      "source": [
        "### Build Model\n",
        "\n",
        "1) Define a model of three dense layers with ReLu activation functions. The output of the two first layers should have 32 neurons. \n",
        "\n",
        "2) Train the model for 2000 epochs with a batch size of 64 and a mean squared error loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlKXS0cMeMcp"
      },
      "source": [
        "## TODO\n",
        "n_epochs = 2000\n",
        "batch_size = 64\n",
        "\n",
        "model = Sequential([...])\n",
        "model.compile(...) # use the Adam optimizer\n",
        "\n",
        "# print a summary of the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRg6dJMwm4VG"
      },
      "source": [
        "## TODO\n",
        "# train the model (use the train data and validation data from above)\n",
        "history = model.fit(..., validation_data=())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-GC0wmLm4VH"
      },
      "source": [
        "## TODO\n",
        "# plot the train and validation losses on the same picture\n",
        "# make sure to label the axis and create a legend "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6kC3IVHYP-E"
      },
      "source": [
        "#### Load the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWgUAW_mYP-F"
      },
      "source": [
        "X_test = pd.read_csv('https://github.com/asarmadi/tandon_summer2021_ml/raw/main/day05/fish_market_test_feature.csv').values\n",
        "y_test = pd.read_csv('https://github.com/asarmadi/tandon_summer2021_ml/raw/main/day05/fish_market_test_label.csv').values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6_ee1QSm4VJ"
      },
      "source": [
        "# scale the test data using the scaler above\n",
        "Xtest_s = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxrXrxmXm4VK"
      },
      "source": [
        "## TODO\n",
        "# predict the corresponding y_hat value of the test dataset (use the scaled test data)\n",
        "y_hat = model.predict(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8hiFtuwYP-P"
      },
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(np.arange(y_hat.shape[0]), y_hat, label='Prediction')\n",
        "plt.scatter(np.arange(y_test.shape[0]), y_test, label='Ground Truth')\n",
        "plt.legend()\n",
        "plt.xlabel('data no.')\n",
        "plt.ylabel('predicted value')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D2GFSHBm4VL"
      },
      "source": [
        "## TODO\n",
        "# print MSE, RMSE (root-mse), MAE on the train and test data\n",
        "# compare these results against last week's results (when we used linear/polynimial regression)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}